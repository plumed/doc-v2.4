<!-- HTML header for doxygen 1.8.3.1-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta name="robots" content="noindex">
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>PLUMED: Trieste tutorial: Averaging, histograms and block analysis</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(initResizable);
/* @license-end */</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table align="center" frame="void" width="98%" cellpadding="2%">
<tbody>
<tr style="height: 30px;">
<td valign="center"> &nbsp; <img src="pigeon.png" width="120"/></td>
<td style="padding-left: 0.2em;" width="74%"> <a href="http://www.plumed.org"> <img src="logo.png" width="400" /> </td>
<td style="padding-left: 0.2em;" align="right"> <a href="../../developer-doc/html/index.html"> <img src="user-logo.png" width="180" /> </a> </td>
</tr>
</tbody>
</table>
<!--
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">PLUMED
   &#160;<span id="projectnumber">2.4.8</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
-->
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('trieste-2.html','');});
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Trieste tutorial: Averaging, histograms and block analysis </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="trieste-2-aim"></a>
Introduction</h1>
<p>The aim of this tutorial is for you to understand how we analyse trajectory data by calculating ensemble averages. One key point we try to make in this tutorial is that you must <b> always </b> calculate averaged quantities from our simulation trajectories. Consequently, in order to understand the analysis that is done when we perform molecular dynamics and Monte Carlo simulations, we will need to understand some things about probability and statistics. In fact, the things that we need to understand about probability and statistics are going to be the main subject of this tutorial. Therefore, in order to make our lives easier, we are not going to work with simulation trajectories in this tutorial. We are going to use model data instead. <br />
 </p>
<h1><a class="anchor" id="trieste-2-lo"></a>
Objectives</h1>
<p>Once this tutorial is completed students will:</p>
<ul>
<li>Be able to explain the role played by the central limit theorem in many of the analyses we perform on simulation trajectories.</li>
<li>Be able to use PLUMED to calculate ensemble averages and histograms using the keywords <a class="el" href="_a_v_e_r_a_g_e.html">AVERAGE</a> and <a class="el" href="_h_i_s_t_o_g_r_a_m.html">HISTOGRAM</a>.</li>
<li>Be able to use PLUMED to perform block analyses of trajectory data using the keywords <a class="el" href="_a_v_e_r_a_g_e.html">AVERAGE</a> and <a class="el" href="_h_i_s_t_o_g_r_a_m.html">HISTOGRAM</a>.</li>
<li>Be able to use PLUMED to calculate unbiased ensemble averages and histograms from biased trajectories by using the keywords <a class="el" href="_a_v_e_r_a_g_e.html">AVERAGE</a>, <a class="el" href="_h_i_s_t_o_g_r_a_m.html">HISTOGRAM</a> and <a class="el" href="_r_e_w_e_i_g_h_t__b_i_a_s.html">REWEIGHT_BIAS</a>.</li>
<li>Be able to explain how block analysis can be used to detect problems with error bar underestimation in correlated data.</li>
</ul>
<h1><a class="anchor" id="trieste-2-introduction"></a>
Background</h1>
<p>Let's begin by thinking about what is actually output by a simulation trajectory. You probably know by now that molecular dynamics gives us a trajectory that provides us with information on how the positions and velocities of the all the atoms in the system change with time. Furthermore, you should by now understand that we can use PLUMED to reduce the ammount of information contained in each of the trajectory frames to a single number or a low-dimensional vector by calculating a collective variable or a set of collective variables. As we saw in <a class="el" href="trieste-1.html">Trieste tutorial: Analyzing trajectories using PLUMED</a> this process of lowering the dimensionality of the data contained in a simulation trajectory was vital as we understand very little by watching the motions of the hundreds of atoms that the system we are simulating contains. If we monitor the appropriate key variables, however, we can determine whether a chemical reaction has taken place, whether the system has undergone a phase transition to a more ordered form or if a protein has folded. Even so if all we do is monitor the values the the collective variables take during the simulation our result can only ever be qualitative and we can only say that we observed this process to take place under these particular conditions on one particular occasion. Obviously, we would like to be able to do more - we would like to be able to make quantiative predictions based on the outcomes of our simulations.</p>
<p>The first step in moving towards making these quantitative predictions involves rethinking what precisely our simulation trajectory has provided us with. We know from rudimentary statistical mechanics that when we perform a molecular dynamics simulation in the NVT ensemble the values of the collective variables that we obtain for each of our trajectory frames, \(X\), are samples from the following probability distribution:</p>
<p class="formulaDsp">
\[ P(s&#39;) = \frac{ \int \textrm{d}x \textrm{d}p \delta( s(x) - s&#39;) e^{-\frac{H(x,p)}{k_B T}} }{ \int \textrm{d}x\textrm{d}p e^{-\frac{H(x,p)}{k_B T}} } \]
</p>
<p>In this expression the integral signs are used to represent \(6N\)-dimensional integrals that run over all the possible positions and momenta that the \(N\) atoms in our system can take. \(H(x,p)\) is the Hamiltonian and \(k_B\) and \(T\) are Boltzmann's constant and the temperature respectively. The quantity calculated by this quotient is the probability that our CV will take a value \(s&#39;\). The quantity in the denominator of the above expression is the canonical partition function while the \(\delta\) function in the integral in the numerator ensures that only those configurations that have a CV value, \(s(x)\), equal to \(s&#39;\) contribute to the integral in the numerator.</p>
<p>The fact that we know what distribution the \(X\)-values obtained from our trajectory frames are taken from is not particularly helpful as it is impossible to calculate the integrals in the expression above analytically. The fact that we know that our trajectory frames represent samples from a distribution is helpful, however, because of a result know as the Central Limit Thoerem. This theorem states the following: </p><p class="formulaDsp">
\[ \lim_{n \rightarrow \infty} P\left( \frac{\frac{S_n}{n} - \langle Y \rangle}{ \sqrt{\frac{\langle(\delta Y)^2\rangle}{n}} } \le z \right) = \Phi(z) \]
</p>
<p> In this expression \(\Phi(z)\) is the cumulative probability distribution function for a standard normal distribution and \(S_n\) is a sum of \(n\) independent samples from a probability distribution - in our case the probability distribution that we introduced in the previous equation. \(\langle Y \rangle\) is the ensemble average for the quantity \(Y(x)\), which, in our case, we can calculate as follows: </p><p class="formulaDsp">
\[ \langle Y \rangle = \frac{ \int \textrm{d}x \textrm{d}p Y(x) e^{-\frac{H(x,p)}{k_B T}} }{ \int \textrm{d}x\textrm{d}p e^{-\frac{H(x,p)}{k_B T}} } \]
</p>
<p> <br />
Lastly, the quantity \(\langle(\delta Y)^2\rangle\) is a measure of the extent of the fluctuations we will observe in the \(Y\) values that we samples. This quantity is calculated using: </p><p class="formulaDsp">
\[ \langle(\delta Y)^2\rangle = \langle Y^2 \rangle - \langle Y \rangle^2 \]
</p>
<p>The statement of the theorem provided above is compact way of stating the formal theorem. This is undoubtedly pleasing to mathematicians but to understand why this idea is so important in the context of molecular simulation it is useful to think about this in a slightly less formal way. The central limit theorem essentially tells us that if we take the set of random variables we extract from a simulation trajectory, which we know represent samples from the complicated distribution in the first equation above, and we add all these random variables together and divide by \(n\) we can think of the final number we obtain as a random variable that is taken from a Gaussian distribution. In other words, the probabiilty density function for the sample mean, \(\frac{S_n}{n}\), that we get from our trajectory frames is given by: </p><p class="formulaDsp">
\[ P(S_n) = \frac{1}{\sqrt{\frac{2\pi \langle(\delta Y)^2\rangle}{n}}} \exp\left( - \frac{\frac{S_n}{n} - \langle Y \rangle}{ 2\frac{\langle(\delta Y)^2\rangle}{n} } \right) \]
</p>
<p> This function is shown plotted for various values of \(n\) in the movie at <a href="https://www.youtube.com/watch?v=-7hlP-2dG_o&feature=youtu.be">https://www.youtube.com/watch?v=-7hlP-2dG_o&amp;feature=youtu.be</a>. <br />
You can see clearly that this distribution becomes more strongly peaked around \(\langle Y\rangle\) (which I set equal to 0 in the movie) as \(n\) increases. <br />
This observation is important as it ensures that the probability that \(\frac{S_n}{n}\) lies close to the true value of the expectation value of our distribution, \(\langle Y\rangle\), increases as we increase the value of \(n\). The central limit theorem therefore allows us to get an estimate for the ensemble average for a particular quantity \(Y\) by taking repeated samples of \(Y\) from our distribution. These samples can be taken by, for example, performing a molecular dynamics simulation. Furthermore, and as we will see in the exercises that follow, we can also get an estimate of how much we might expect the system to fluctuate about this average. Incidentally, if you are confused at this stage you might want to work through these two videos and exercises in order to get a better understanding of the central limit theorem, confidence limits and error bars:</p>
<ul>
<li>Error bars exercise: <a href="http://gtribello.github.io/mathNET/error_bar_video.html">http://gtribello.github.io/mathNET/error_bar_video.html</a></li>
<li>Confidence limits exercise: <a href="http://gtribello.github.io/mathNET/central-limit-theorem-video.html">http://gtribello.github.io/mathNET/central-limit-theorem-video.html</a></li>
</ul>
<h1><a class="anchor" id="trieste-2-instructions"></a>
Instructions</h1>
<h2><a class="anchor" id="trieste-2-average"></a>
Calculating an ensemble average</h2>
<p>As discussed in the introduction we are going to be using model data in this exercise so we must begin by generating some model data to analyse using PLUMED. The following short python script will generate 10000 (pseudo) random variables between 0 and 1 from a uniform disribution in a format that PLUMED can understand:</p>
<div class="fragment"><div class="line"><span class="keyword">import</span> random</div><div class="line"></div><div class="line">print(<span class="stringliteral">&quot;#! FIELDS time rand&quot;</span>)</div><div class="line"><span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(0,10001):</div><div class="line">        print(i, random.uniform(0,1) )</div></div><!-- fragment --><p>Copy the contents of the box above to a plain text file called generate_data.py, save the file and then execute the script within it by running:</p>
<pre class="fragment">&gt; python generate_data.py &gt; mydata
</pre><p>This will generate a file called mydata that contains 10001 uniform random variables. PLUMED will ignore the first number in the colvar file as it assumes this is the initial configuration you provided for the trajectory. The sample mean will thus be calculated from 10000 random variables. Plot this data now using gnuplot and the command:</p>
<pre class="fragment">gnuplot&gt; p 'mydata' u 1:2 w p
</pre><p>The probability distribution that we generated these random variables is considerably simpler than the probability distribution that we would typically sample from during an MD simulation. Consequently, we can calculate the exact values for the ensemble average and the fluctuations for this distribution. These are:</p>
<p class="formulaDsp">
\[ \begin{aligned} \langle X \rangle &amp;= \int_0^1 x \textrm{d}x = \left[ \frac{x^2}{2} \right]_0^1 = \frac{1}{2} \\ \langle X^2 \rangle &amp;= \int_0^1 x^2 \textrm{d}x = \left[ \frac{x^3}{3} \right]_0^1 = \frac{1}{3} \\ \langle (\delta X)^2 \rangle &amp; = \langle X^2 \rangle - \langle X \rangle^2 = \frac{1}{12} \end{aligned} \]
</p>
<p>Lets now try estimating these quantieis by calculating \(\frac{S_n}{n}\) from the points we generated and exploting the central limit theorem. We can do this calculation by writing a PLUMED input file that reads:</p>
<pre class="fragment">
data: <a href="./_r_e_a_d.html" style="color:green">READ</a> FILE=mydata VALUES=rand
d2: <a href="./_m_a_t_h_e_v_a_l.html" style="color:green">MATHEVAL</a> ARG=data VAR=a FUNC=a*a PERIODIC=NO
av: <a href="./_a_v_e_r_a_g_e.html" style="color:green">AVERAGE</a> ARG=data STRIDE=1
av2: <a href="./_a_v_e_r_a_g_e.html" style="color:green">AVERAGE</a> ARG=d2 STRIDE=1
<a href="./_p_r_i_n_t.html" style="color:green">PRINT</a> ARG=av,av2 STRIDE=10000 FILE=colvar
</pre><p>If you copy this input to a file called plumed.dat you can then run the calculation by executing:</p>
<pre class="fragment">&gt; plumed driver --noatoms
</pre><p>When the calculation is finished you should have a file called colvar that contains the estimate of the ensemble averages \(\langle X \rangle\) and \(\langle X^2 \rangle\). To be clear, the quantities output in this file are:</p>
<p class="formulaDsp">
\[ \langle X \rangle = \frac{1}{N} \sum_{i=1}^N X_i \qquad \textrm{and} \qquad \langle X^2 \rangle \frac{1}{N} \sum_{i=1}^N X_i^2 \]
</p>
<p>We can calculate the flucutations, \((\delta X)^2\), from them using:</p>
<p class="formulaDsp">
\[ (\delta X)^2 = \langle X^2 \rangle - \langle X \rangle^2 \]
</p>
<p>We can then compare the values that we got for these estimated values with those that we got for the true values. You should find that the agreement is reasonable but not perfect.</p>
<h2><a class="anchor" id="trieste-2-histo"></a>
Calculating a histogram</h2>
<p>We can use what we have learnt about calculating an ensemble average to calculate an estimate for the probability density function or probability mass function for our random variable. The theory behind what we do here is explained in this video <a href="http://gtribello.github.io/mathNET/histogram-video.html">http://gtribello.github.io/mathNET/histogram-video.html</a></p>
<p>To do such a calculation with PLUMED on the random variables we generated from the uniform distribution in the previous section we would use an input like the one below:</p>
<pre class="fragment">
data: <a href="./_r_e_a_d.html" style="color:green">READ</a> FILE=mydata VALUES=rand
hh: <a href="./_h_i_s_t_o_g_r_a_m.html" style="color:green">HISTOGRAM</a> ARG=data STRIDE=1 GRID_MIN=0 GRID_MAX=1.0 GRID_BIN=20 KERNEL=DISCRETE
<a href="./_d_u_m_p_g_r_i_d.html" style="color:green">DUMPGRID</a> GRID=hh FILE=myhist.dat
</pre><p>Once again you can run this calculation by using the command:</p>
<pre class="fragment">&gt; plumed driver --noatoms
</pre><p>Once this calculation is completed you can then plot the output using gnuplot and the command:</p>
<pre class="fragment">gnuplot&gt; p 'myhist.dat' u 1:2 w l
</pre><p>In the previous section we compared the estimates we got for the ensemble average with the exact analytical values, which we could determine because we know that the data was sampled from a uniform distribution between 0 and 1. We can do a similar thing with the histogram. <b> Can you determine what the true (analytic) values for the probabilities in the histogram above should be? </b> <br />
Also notice that the probability estimate for the first and last points in our histogram are lower than the probability estimates for the other points. <br />
<b> Can you determine why these two points have a lower probability? </b></p>
<h2><a class="anchor" id="trieste-2-blocks"></a>
Problem I: Making the best use of the data</h2>
<p>As discussed in previous sections the sample mean of the CV values from our trajectory frames is a random variable and the central limit theorem tells us something about the the <b> distribution </b> from which this random variable is drawn. The fact that we know what distribution the sample mean is drawn from is what allows us to <b> estimate </b> the ensemble average and the fluctuations. The fact that this recipe is only estimating the ensemble average is critical and this realisation should always be at the forefront of our minds whenever we analyse our simulation data. The point, once again, is that the sample mean for CV values from the simulation is random. As is explained in this video <a href="http://gtribello.github.io/mathNET/central-limit-theorem-video.html,">http://gtribello.github.io/mathNET/central-limit-theorem-video.html,</a> however, we can use the central limit theorem to calculate a range, \(\{\langle X \rangle-\epsilon,\langle X \rangle+\epsilon\}\), that the sample mean for the CV values, \(\frac{S_n}{n}\), will fall into with a probability \(p_c\) using: </p><p class="formulaDsp">
\[ \epsilon = \sqrt{ \frac{\langle ( \delta X )^2 \rangle }{n} } \Phi^{-1}\left( \frac{p_c + 1}{2} \right) \]
</p>
<p> Here \(\Phi^{-1}\) is the inverse of the cumulative probability distribution function for a normal distribution with mean 0 and variance 1. As you can see this range gets smaller as the number of samples from which you calculate the mean, \(n\), increases. As is shown in the figure below, however, the rate at which this range narrows in size is relatively small. <br />
 <a class="anchor" id="triest-2-confidence"></a></p><div class="image">
<img src="trieste-2-confidence.png" alt="trieste-2-confidence.png"/>
<div class="caption">
A figure showing how the width of the 90% confidence limit changes with number of samples. Two lines are shown - one at plus epsilon and one at minus epsilon. This figure shows clearly that simply averaging over more random variables will not get us markedly closer to the true value of the ensemble average, which is 0 for this particular example. For this random variable the average fluctuation is equal to one.</div></div>
<p> This graph hopefully illustrates to you that an estimate of the ensemble average that is taken over 500 trajectory frames is not guaranteed to lie significantly closer to the true ensemble average than an estimate taken from 100 trajectory frames. For this reason, we might choose to split the data into blocks that all have equal length. Will will then estimate the average for each of these blocks separately. As we will see in the remainder of this exercise this process of block averaging has a number of other advantages. For now though we are just going to use it to test that the results from the various parts of ``the trajectory" are all consistent.</p>
<p>We can perform a block averaging on the data we generated at the start of the first exercise above by using PLUMED and the input file below:</p>
<pre class="fragment">
data: <a href="./_r_e_a_d.html" style="color:green">READ</a> FILE=mydata VALUES=rand
av: <a href="./_a_v_e_r_a_g_e.html" style="color:green">AVERAGE</a> ARG=data STRIDE=1 CLEAR=1000
<a href="./_p_r_i_n_t.html" style="color:green">PRINT</a> ARG=av STRIDE=1000 FILE=colvar
</pre><p>Once again this calculation can be executed by running the command:</p>
<pre class="fragment">&gt; plumed driver --noatoms
</pre><p>The key differences between this input and the ones that we have seen previously is the CLEAR keyword on the line AVERAGE. The intruction CLEAR=1000 tells PLUMED that the data that has been accumulated for averaging should be reset to zero (cleared) every 1000 steps. If you look at the subsequent PRINT command in the above input you see the instruction STRIDE=1000. The above input thus accumulates an average over 1000 trajectory frames. This average is then output to the file colvar on step 1000 and the accumulated data is immediately cleared after printing so that a new average over the next 1000 steps can be accumulated. We can plot the averages that were output from the calculation above by using gnuplot and the following command:</p>
<pre class="fragment">gnuplot&gt; p 'colvar' u 1:2 w l
</pre><p>If you try this now you should see that all the average values that were calculated are relatively consistent but that there are differences between them. <b> Try to calculate the size of \(\epsilon\) for a 90 % confidence limit around this random variable using the formula that was provided above. How many of the averages that you exptracted using PLUMED lie within this range? Is this behavior inline with your expectations based on your understanding of what a confidence limit is? </b></p>
<p>We can also perform block averaging when we estimate histograms using PLUMED. The following input will calculate these block averaged histograms for the data we generated at the start of this exercise using PLUMED.</p>
<pre class="fragment">
data: <a href="./_r_e_a_d.html" style="color:green">READ</a> FILE=mydata VALUES=rand
hh: <a href="./_h_i_s_t_o_g_r_a_m.html" style="color:green">HISTOGRAM</a> ARG=data STRIDE=1 GRID_MIN=0 GRID_MAX=1.0 GRID_BIN=20 KERNEL=DISCRETE CLEAR=1000
<a href="./_d_u_m_p_g_r_i_d.html" style="color:green">DUMPGRID</a> GRID=hh FILE=myhist.dat STRIDE=1000
</pre><p>Notice that the input here has the same structure as the input for the <a class="el" href="_a_v_e_r_a_g_e.html">AVERAGE</a>. Once again we have a CLEAR=1000 keyword that tells PLUMED that the data that has been accumulated for calculating the histogram should be deleted every 1000 steps. In addition, we can set a STRIDE for <a class="el" href="_d_u_m_p_g_r_i_d.html">DUMPGRID</a> and thus output the histogram from each of these blocks of trajectory data separately. The difference between the output from this input and the output from the input above is that in this case we have multiple output files. In particular, the input above should give you 10 output files which will be called:</p>
<ul>
<li>analysis.0.myhist.dat = analysis of data in first 1000 frames</li>
<li>analysis.1.myhist.dat = analysis of data in second 1000 frames</li>
<li>analysis.2.myhist.dat = analysis of data in third 1000 frames</li>
<li>analysis.3.myhist.dat = analysis of data in fourth 1000 frames</li>
<li>analysis.4.myhist.dat = analysis of data in fifth 1000 frames</li>
<li>analysis.5.myhist.dat = analysis of data in sixth 1000 frames</li>
<li>analysis.6.myhist.dat = analysis of data in seventh 1000 frames</li>
<li>analysis.7.myhist.dat = analysis of data in eigth 1000 frames</li>
<li>analysis.8.myhist.dat = analysis of data in ninth 1000 frames</li>
<li>myhist.dat = analysis of data in tenth 1000 frames</li>
</ul>
<p>We can plot all of these histograms using gnuplot and the command:</p>
<pre class="fragment">gnuplot&gt; p 'analysis.0.myhist.dat' u 1:2 w l, 'analysis.1.myhist.dat' u 1:2 w l, 'analysis.2.myhist.dat' u 1:2 w l, 'analysis.3.myhist.dat' u 1:2 w l, 'analysis.4.myhist.dat' u 1:2 w l, 'analysis.5.myhist.dat' u 1:2 w l, 'analysis.6.myhist.dat' u 1:2 w l, 'analysis.7.myhist.dat' u 1:2 w l, 'analysis.8.myhist.dat' u 1:2 w l, 'myhist.dat' u 1:2 w l
</pre><p>Performing a comparison between the results from each of these blocks of data is more involved than the analysis we performed when comparing the ensemble averages as we have more data. The essential idea is the same, however, and, if you have time at the end of the session, you might like to see if you can write a script that determines what fraction of the many averages we have calculated here lie within the confidence limits.</p>
<h2><a class="anchor" id="trieste-2-biases"></a>
Problem II: Dealing with rare events and simulation biases</h2>
<p>As is discussed in many of the tutorials that are provided here one of PLUMED's primary purposes is to use simulation biases to resolve the rare events problem. When we use these technique we modify the Hamiltonian, \(H(x,p)\), and add to it some bias, \(V(x)\). The modified Hamiltonian thus becomes: </p><p class="formulaDsp">
\[ H&#39;(x,p) = H(x,p) + V(x) \]
</p>
<p> and as such the values of the collective variables that we obtain from each of our trajectory frames are samples from the following distribution: </p><p class="formulaDsp">
\[ P(s&#39;) = \frac{ \int \textrm{d}x \textrm{d}p \delta( s(x) - s&#39;) e^{-\frac{H(x,p)}{k_B T}}e^{-\frac{V(x)}{k_B T}} }{ \int \textrm{d}x\textrm{d}p e^{-\frac{H(x,p)}{k_B T}}e^{-\frac{V(x)}{k_B T}} } \]
</p>
<p> Using a bias in this way is enormously helpful as we can ensure that we sample from a particular part of configuration space. We appear to have sacrificed, however, the ability to extract estimates of ensemble averages for the unbiased distribution using the central limit theorem. If we calculate the mean from a set of trajectory frames that are sampled from the distribution above we will get an estimate for the ensemble average in this biased distribution. As we will see in this exercise, however, this is not really a problem as we can use reweighting techniques to extract ensemble averages for the unbiased distribution. <br />
 Lets begin by generating some new trajectory data. The following python script generates a set of random variables from a (truncated) normal distribution with \(\sigma=0.5\) and \(\mu=0.6\). In other words, points are generated from the following probability distribution but if they don't fall in a range between 0 and 1 they are discarded:</p>
<p class="formulaDsp">
\[ P(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right) \]
</p>
<div class="fragment"><div class="line"><span class="keyword">import</span> random</div><div class="line"></div><div class="line">n = 0</div><div class="line">print(<span class="stringliteral">&quot;#! FIELDS time rand&quot;</span>)</div><div class="line"><span class="keywordflow">while</span> <span class="keyword">True</span> :</div><div class="line">        x = random.gauss( 0.6, 0.1732 )</div><div class="line">        <span class="keywordflow">if</span> (x&gt;=0) &amp; (x&lt;=1) :</div><div class="line">                print(n, x )</div><div class="line">                n = n + 1</div><div class="line">        <span class="keywordflow">if</span> n==10001 : <span class="keywordflow">break</span> </div></div><!-- fragment --><p>Copy the script above to a file called gen-normal.py and then execute the python script within it using the command:</p>
<pre class="fragment">&gt; python gen-normal.py &gt; mynormal
</pre><p><b> Use what you have learnt from the exercises above to estimate the ensemble average from these generated data points using PLUMED. If you want you can use block averaging but it doenst matter if you do it by just considering all the data in the input file. What would you expect the ensemble average to be and how does the estimate you get from PLUMED compare with the true value? </b></p>
<p>You should have found that the ensemble average that you get when you perform the experiment described in the previous paragraph is different from the ensemble average that you get when you considered the uniform distribution. This makes sense as the distribution we sampled from here is approximately:</p>
<p class="formulaDsp">
\[ P(x) = \frac{1}{0.03*\sqrt{2\pi}} \exp\left( - \frac{ x - 0.6}{ 2(0.03)^2} \right) \]
</p>
<p>This is different from the distribution we sampled from in the previous exercises. A question we might therefore ask is: can we extract the ensemble average for the uniform distribution that we sampled in previous exercises by sampling from this different distribution? In the context of the experiment we are performing here with the random variables this question perhaps feels somewhat absurd and pointless. In the context of molecular simulation, however, answering this question is essential as our ability to extract the true, unbiased distribution from a simulation run with a bias relies on being able to perform this sort of reweighting.</p>
<p>The true value of the ensemble average, which we estimated when we analysed the data generated from the Gaussin using the python script above is given by:</p>
<p class="formulaDsp">
\[ \langle X \rangle = \frac{ \int_0^1 x e^{-V(x)} \textrm{d}x }{\int_0^1 e^{-V(x)} \textrm{d}x } \qquad \textrm{where} \qquad V(x) = \frac{ (x - x)^2}{2\sigma^2} \]
</p>
<p>By contrast the ensemble average in the exercises involving the uniform distribution is given by:</p>
<p class="formulaDsp">
\[ \langle Y \rangle = \frac{ \int_0^1 y \textrm{d}y }{ \int_0^1 \textrm{d}y } = \frac{ \int_0^1 y e^{-V(y)}e^{+V(y)} \textrm{d}y }{\int_0^1 e^{-V(y)}e^{+V(y)} \textrm{d}y } \]
</p>
<p>We can use the final expression here to reweight the data that we sampled from the Gaussian. By doing so can thus extract ensemble averages for the uniform distribution. The trick here is calculate the following weighted mean rather than the unweighted mean that we calculated previously:</p>
<p class="formulaDsp">
\[ \langle Y \rangle \approx \frac{\sum_i Y_i e^{+V(Y_i)}}{ \sum_i e^{+V(Y_i)} } \]
</p>
<p> <br />
 where here the sums run over all the random variables, the \(Y_i\)s, that we sampled from the Gaussian distribution. If you used the script above the \(V(Y_i)\) values were output for you so you can calculate this estimate of the ensemble average for the unbiased distribution in this case using the following PLUMED input.</p>
<pre class="fragment">
<a href="./_u_n_i_t_s.html" style="color:green">UNITS</a> NATURAL  <span style="color:blue"># This ensures that Boltzmann's constant is one</span>
data: <a href="./_r_e_a_d.html" style="color:green">READ</a> FILE=mynormal VALUES=rand IGNORE_FORCES
mm: <a href="./_r_e_s_t_r_a_i_n_t.html" style="color:green">RESTRAINT</a> ARG=data AT=0.6 KAPPA=33.333
rw: <a href="./_r_e_w_e_i_g_h_t__b_i_a_s.html" style="color:green">REWEIGHT_BIAS</a> TEMP=1
av: <a href="./_a_v_e_r_a_g_e.html" style="color:green">AVERAGE</a> ARG=data STRIDE=1 LOGWEIGHTS=rw
<a href="./_p_r_i_n_t.html" style="color:green">PRINT</a> ARG=av STRIDE=10000 FILE=colvar
</pre><p>Try to run this calculation now using:</p>
<pre class="fragment">&gt; plumed driver --noatoms 
</pre><p>and see how close you get to the ensemble average for the uniform distribution. Once you have done this try the following input, which allows you to compute the reweighted histogram.</p>
<pre class="fragment">
<a href="./_u_n_i_t_s.html" style="color:green">UNITS</a> NATURAL <span style="color:blue"># This ensures that Boltzmann's constant is one</span>
data: <a href="./_r_e_a_d.html" style="color:green">READ</a> FILE=mynormal VALUES=rand IGNORE_FORCES
mm: <a href="./_r_e_s_t_r_a_i_n_t.html" style="color:green">RESTRAINT</a> ARG=data AT=0.6 KAPPA=33.333
rw: <a href="./_r_e_w_e_i_g_h_t__b_i_a_s.html" style="color:green">REWEIGHT_BIAS</a> TEMP=1
hh: <a href="./_h_i_s_t_o_g_r_a_m.html" style="color:green">HISTOGRAM</a> ARG=data STRIDE=1 GRID_MIN=0 GRID_MAX=1.0 GRID_BIN=20 KERNEL=DISCRETE LOGWEIGHTS=rw
<a href="./_d_u_m_p_g_r_i_d.html" style="color:green">DUMPGRID</a> GRID=hh FILE=myhist.dat 
</pre><p>Plot the histogram that you obtain from this calculation using the command:</p>
<pre class="fragment">gnuplot&gt; p 'myhist.dat' w l 
</pre><p><b> <br />
Now suppose that the data we generated for this exercise had come from an MD simulation. What would the unbiased Hamiltonian in this MD simulation have looked like and what functional form would our simulation bias have taken? <br />
</b></p>
<h2><a class="anchor" id="triete-2-correlation"></a>
Problem III: Dealing with correlated variables</h2>
<p>As a good scientist you have probably noticed that we have failed to provide error bars around our estimates for the ensemble average in previous sections. Furthermore, you may have found that rather odd given that I showed you how to estimate the variance in the data in the very first exericse. This ignoring of the error bars has been deliberate, however, as there is a further problem with the data that comes from molecular dynamics trajectories that we have to learn to deal with and that will affect our ability to esimtate the error bars. This problem is connected to the fact that <b>the central limit theorem only holds when we take a sum of uncorrelated and identical random variables.</b> Random variables that are generated from simulation trajectories will contain correlations simply because the system will most likely not diffuse from one edge of CV space to the other during a single timestep. In other words, the random CV value that we calcuate from the \((i+1)\)th frame of the trajectory will be similar to the value obtained from the \(i\)th trajectory frame. This problem can be resolved, however, and to show how we will thus use the following python script to generate some correlated model data:</p>
<div class="fragment"><div class="line"><span class="keyword">import</span> random</div><div class="line"></div><div class="line">prev = 0.;</div><div class="line">print(<span class="stringliteral">&quot;#! FIELDS time rand&quot;</span>)</div><div class="line"><span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(0,10001):</div><div class="line">        new = 0.95*prev + 2*random.uniform(0,1) - 1</div><div class="line">        print( i, new/2. + 0.5 )</div><div class="line">        prev = new</div></div><!-- fragment --><p>Copy the python script above to a filled called correlated-data.py and then execute the script using the command:</p>
<pre class="fragment">&gt; python correlated-data.py &gt; mycorr
</pre><p>The autocorrelation function, \(R(\tau)\) provides a simple method for determining whether or not there are correlations between the random variables, \(X\), in our time series. This function is defined as follows:</p>
<p class="formulaDsp">
\[ R(\tau) = \frac{ \langle (X_t - \langle X \rangle )(X_{t+\tau} - \langle X \rangle ) \rangle }{ \langle (\delta X)^2 \rangle } \]
</p>
<p>At present it is not possible to calculate this function using PLUMED. If we were to do so for the samples taken from the uniform distribution and the correlated samples that were taken from the distribution the python script above we would find that the auto correlation functions for these random variables look something like the figures shown below:</p>
<p><a class="anchor" id="triest-2-autocorrelation"></a></p><div class="image">
<img src="trieste-2-autocorrelation.png" alt="trieste-2-autocorrelation.png"/>
<div class="caption">
The autocorrelation functions for the data that was generated by taking samples from a uniform distribution (left panel) and the autocorrelation function for the correlated data that was generated using the python script in this section (right panel)</div></div>
<p> To understand what these functions are telling us lets deal with the samples from the uniform distribution first. The autocorrelation function in this case has a value of 1 when \(\tau\) is equal to 0 and a value of 0 in all other cases. This function thus tells us that each random variable is perfectly correlated with itself but that there are no correlations between the distributions we sampled adjacent variables from. In other words, each of the random variables we generate are independent. If we now look at the autocorrelation function for the distribution that is sampled by the python script above we see that the autocorrelation function slowly decays to zero. There are, therefore, correlations between the random variables that we generate that we must account for when we perform our analysis.</p>
<p>We account for the correlations in the data when we do our analysis by performing a block analysis. To understand what precisely this involves we are going to perform the analysis with PLUMED and explain the results that we get at each stage of the process. We wil begin by analysing the data we generated by sampling from the uniform distribution using the following PLUMED input:</p>
<pre class="fragment">
data: <a href="./_r_e_a_d.html" style="color:green">READ</a> FILE=mydata VALUES=rand
av5: <a href="./_a_v_e_r_a_g_e.html" style="color:green">AVERAGE</a> ARG=data STRIDE=1 CLEAR=5
<a href="./_p_r_i_n_t.html" style="color:green">PRINT</a> ARG=av5 FILE=colvar5 STRIDE=5
av10: <a href="./_a_v_e_r_a_g_e.html" style="color:green">AVERAGE</a> ARG=data STRIDE=1 CLEAR=10
<a href="./_p_r_i_n_t.html" style="color:green">PRINT</a> ARG=av10 FILE=colvar10 STRIDE=10
av15: <a href="./_a_v_e_r_a_g_e.html" style="color:green">AVERAGE</a> ARG=data STRIDE=1 CLEAR=15
<a href="./_p_r_i_n_t.html" style="color:green">PRINT</a> ARG=av15 FILE=colvar15 STRIDE=15
av20: <a href="./_a_v_e_r_a_g_e.html" style="color:green">AVERAGE</a> ARG=data STRIDE=1 CLEAR=20
<a href="./_p_r_i_n_t.html" style="color:green">PRINT</a> ARG=av20 FILE=colvar20 STRIDE=20
av25: <a href="./_a_v_e_r_a_g_e.html" style="color:green">AVERAGE</a> ARG=data STRIDE=1 CLEAR=25
<a href="./_p_r_i_n_t.html" style="color:green">PRINT</a> ARG=av25 FILE=colvar25 STRIDE=25
av30: <a href="./_a_v_e_r_a_g_e.html" style="color:green">AVERAGE</a> ARG=data STRIDE=1 CLEAR=30
<a href="./_p_r_i_n_t.html" style="color:green">PRINT</a> ARG=av30 FILE=colvar30 STRIDE=30
av35: <a href="./_a_v_e_r_a_g_e.html" style="color:green">AVERAGE</a> ARG=data STRIDE=1 CLEAR=35
<a href="./_p_r_i_n_t.html" style="color:green">PRINT</a> ARG=av35 FILE=colvar35 STRIDE=35
av40: <a href="./_a_v_e_r_a_g_e.html" style="color:green">AVERAGE</a> ARG=data STRIDE=1 CLEAR=40
<a href="./_p_r_i_n_t.html" style="color:green">PRINT</a> ARG=av40 FILE=colvar40 STRIDE=40
av45: <a href="./_a_v_e_r_a_g_e.html" style="color:green">AVERAGE</a> ARG=data STRIDE=1 CLEAR=45
<a href="./_p_r_i_n_t.html" style="color:green">PRINT</a> ARG=av45 FILE=colvar45 STRIDE=45
av50: <a href="./_a_v_e_r_a_g_e.html" style="color:green">AVERAGE</a> ARG=data STRIDE=1 CLEAR=50
<a href="./_p_r_i_n_t.html" style="color:green">PRINT</a> ARG=av50 FILE=colvar50 STRIDE=50
av55: <a href="./_a_v_e_r_a_g_e.html" style="color:green">AVERAGE</a> ARG=data STRIDE=1 CLEAR=55
<a href="./_p_r_i_n_t.html" style="color:green">PRINT</a> ARG=av55 FILE=colvar55 STRIDE=55
av60: <a href="./_a_v_e_r_a_g_e.html" style="color:green">AVERAGE</a> ARG=data STRIDE=1 CLEAR=60
<a href="./_p_r_i_n_t.html" style="color:green">PRINT</a> ARG=av60 FILE=colvar60 STRIDE=60
av65: <a href="./_a_v_e_r_a_g_e.html" style="color:green">AVERAGE</a> ARG=data STRIDE=1 CLEAR=65
<a href="./_p_r_i_n_t.html" style="color:green">PRINT</a> ARG=av65 FILE=colvar65 STRIDE=65
av70: <a href="./_a_v_e_r_a_g_e.html" style="color:green">AVERAGE</a> ARG=data STRIDE=1 CLEAR=70
<a href="./_p_r_i_n_t.html" style="color:green">PRINT</a> ARG=av70 FILE=colvar70 STRIDE=70
</pre><p>Copy the input above to a file called plumed.dat and run the calculation using the command:</p>
<pre class="fragment">&gt; plumed driver --noatoms
</pre><p>This calculation should output 14 colvar files, which should then be further analysed using the following python script:</p>
<div class="fragment"><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> math</div><div class="line"></div><div class="line"><span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(1,15):</div><div class="line">        <span class="comment"># Read in each colvar file</span></div><div class="line">        fmult = 5*i</div><div class="line">        dat = np.loadtxt( <span class="stringliteral">&#39;colvar&#39;</span> + str(fmult) )</div><div class="line">        <span class="comment"># Compute the square of all the average values in the colvar</span></div><div class="line">        sq = dat[1:,1]**2</div><div class="line">        <span class="comment"># Now compute the average over all the averages</span></div><div class="line">        mean = np.sum( dat[1:,1] ) / len( dat[1:,1] )</div><div class="line">        <span class="comment"># Compute the average of the squares of the individual averages </span></div><div class="line">        mean2 = np.sum( sq ) / len( sq )</div><div class="line">        <span class="comment"># Compute the population variance amongst the block averages</span></div><div class="line">        population_variance = mean2 - mean*mean</div><div class="line">        <span class="comment"># Convert the population variance into a sample variance by multiplying by the bessel factor</span></div><div class="line">        sample_variance = ( len( sq ) / ( len(sq) - 1 ) )*population_variance</div><div class="line">        <span class="comment"># Print out the length of the blocks, the final average taken over all blocks and the square </span></div><div class="line">        <span class="comment"># root of the sample variance divided by the number of data points that this estimate was </span></div><div class="line">        <span class="comment"># calcualted from.  This last term is a measure of the eror bar</span></div><div class="line">        print( fmult, mean, math.sqrt( sample_variance / len(sq) ) )</div></div><!-- fragment --><p>Copy this script to a file called block-average-script.py and then execute the contents using the command:</p>
<pre class="fragment">&gt; python block-average-script.py &gt; myaverages.dat
</pre><p>This will output a single file called myaverages.dat that can be plotted using gnuplot and the command:</p>
<pre class="fragment">gnuplot&gt; p 'myaverages.dat' u 1:2:3 w e, 'myaverages.dat' w l
</pre><p>The final result should be a graph like that shown in the left panel of the figure below. The figure on the right shows what you should obtain when you repeat the same analysis on the correlated data that we generated using the python script in this section.</p>
<p><a class="anchor" id="triest-2-block-average"></a></p><div class="image">
<img src="trieste-2-block-averages.png" alt="trieste-2-block-averages.png"/>
<div class="caption">
The ensemble average and an estimate of the associated error bars calculated for different lengths of block average. The left panel shows the output obtained when the uncorrelated samples taken from uniform distribution are analysed in this way. The right panel shows the output obtained when the correlated samples that are generated using the python script in this section are analysed.</div></div>
<p> <b> The output that you obtain from these two calculations is explained in the video at: <a href="http://gtribello.github.io/mathNET/block_averaging_video.html">http://gtribello.github.io/mathNET/block_averaging_video.html</a> Before wathcing this explanation though take some time to note down the differences between the two graphs above. Try to look through the scripts above and to understand what is being done in the PLUMED inputs and the python scripts above so as to work out what the data in these two figures are telling you. <br />
</b></p>
<h2><a class="anchor" id="triete-2-alltogether"></a>
Putting it all together</h2>
<p>In this final exercise we are going to try to combine everything we have seen in the previous sections. We are going to sample from the distribution that was introduced in <a class="el" href="trieste-2.html#trieste-2-biases">Problem II: Dealing with rare events and simulation biases</a>. This time though we are not going to generate random variables from the Gaussian directly. We are instead going to use Monte Carlo sampling. This sampling method is going to give us correlated data so we will need to use ideas from <a class="el" href="trieste-2.html#triete-2-correlation">Problem III: Dealing with correlated variables</a> in order to get a proper estimate of the error bars. Furthermore, we are not going to try to extract ensemble averages that tell us about the distribution we sampled from. Instead we are going to reweight using the ideas from <a class="el" href="trieste-2.html#trieste-2-biases">Problem II: Dealing with rare events and simulation biases</a> and extract the unbiased distribution.</p>
<p>The first step in doing all this is, as always, to generate some data. The python script below will generate this data:</p>
<div class="fragment"><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="keyword">import</span> random</div><div class="line"></div><div class="line"><span class="comment"># Energy given by a harmonic potential centered at 0.6</span></div><div class="line"><span class="comment"># This ensures that our data represent samples from a Gaussian with</span></div><div class="line"><span class="comment"># mean 0.6 and variance 0.1732</span></div><div class="line"><span class="keyword">def </span>calc_eng( x ) :</div><div class="line">        <span class="keywordflow">return</span> 0.5*33.333*pow((x-0.6),2)</div><div class="line"></div><div class="line">x = 0.5</div><div class="line">eng = calc_eng( x )</div><div class="line">print(<span class="stringliteral">&quot;#! FIELDS time rand&quot;</span>)</div><div class="line"><span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(0,100010):</div><div class="line">        <span class="comment"># Generate new random position from old position</span></div><div class="line">        newx = x + 0.1*random.uniform(0,1) - 0.05</div><div class="line">        <span class="comment"># Apply periodic boundary conditions</span></div><div class="line">        if( newx &gt; 1.0 ) : newx = newx - 1.0</div><div class="line">        if( newx &lt; 0.0 ) : newx = newx + 1.0</div><div class="line">        <span class="comment"># Monte Carlo criterion</span></div><div class="line">        new_eng = calc_eng( newx )</div><div class="line">        if( new_eng&lt;eng ) :</div><div class="line">                x, eng = newx, new_eng</div><div class="line">        elif( random.uniform(0,1)&lt;math.exp(-new_eng)/math.exp(-eng) ) :</div><div class="line">                x, eng = newx, new_eng</div><div class="line">        if( i%10==0 ) : print( i/10, x )</div></div><!-- fragment --><p>Copy this script to a filled called do-monte-carlo.py and execute the contents of the script using the command:</p>
<pre class="fragment">&gt; python do-monte-carlo.py &gt; mcdata
</pre><p>This will run a short Monte Carlo simulation that generates (time-correlated) random data from a (roughly) Gaussian distribution by attempting random translational moves of up to 0.1 units. An autocorrelation function that was calculated using data generated using this script is shown below. You can clearly see from this figure that there are correlations between the adjacent data points in the time series and that we have to do block averaging as a result.</p>
<p><a class="anchor" id="triest-2-mc-autocorr"></a></p><div class="image">
<img src="trieste-2-mc-autocorrelation.png" alt="trieste-2-mc-autocorrelation.png"/>
<div class="caption">
The autocorrelation function for the data that was generated using the Monte Carlo sampler in the script above</div></div>
<p> <br />
 We can use the following PLUMED input to calculate block averages for the unbiased histogram from the data we generated:</p>
<pre class="fragment">
<a href="./_u_n_i_t_s.html" style="color:green">UNITS</a> NATURAL <span style="color:blue"># This ensures that Boltzmann's constant is one</span>
data: <a href="./_r_e_a_d.html" style="color:green">READ</a> FILE=mcdata VALUES=rand IGNORE_FORCES
mm: <a href="./_r_e_s_t_r_a_i_n_t.html" style="color:green">RESTRAINT</a> ARG=data AT=0.6 KAPPA=33.333
rw: <a href="./_r_e_w_e_i_g_h_t__b_i_a_s.html" style="color:green">REWEIGHT_BIAS</a> TEMP=1
hh: <a href="./_h_i_s_t_o_g_r_a_m.html" style="color:green">HISTOGRAM</a> ARG=data STRIDE=1 GRID_MIN=0 GRID_MAX=1.0 GRID_BIN=20 KERNEL=DISCRETE LOGWEIGHTS=rw CLEAR=500
<a href="./_d_u_m_p_g_r_i_d.html" style="color:green">DUMPGRID</a> GRID=hh FILE=myhist.dat STRIDE=500
</pre><p>Notice that this input instructs PLUMED to calculate block averages for the histogram from each set of 500 consecutive frames in the trajectory. <br />
I have worked out that this is an appropriate length of time to average over by performing the analysis described in <a class="el" href="trieste-2.html#triete-2-correlation">Problem III: Dealing with correlated variables</a>. <br />
We will come back to how precisely I did this momentarily, however. For the time being though you can execute this input using:</p>
<pre class="fragment">&gt; plumed driver --noatoms
</pre><p> <br />
 Executing this command will generate a number of files containing histograms. The following python script will merge all this data and calculate the final histogram together with the appropriate error bars.</p>
<div class="fragment"><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="keyword">import</span> glob</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># Here are some numbers you will need to change if you run this script on grids generated in different contexts</span></div><div class="line">nquantities = 1          <span class="comment"># Number of quanities that have been evaluated on the grid</span></div><div class="line">grid_dimension = 1       <span class="comment"># Number of collective variables that you provided using the ARG keyword</span></div><div class="line">filename = <span class="stringliteral">&quot;myhist.dat&quot;</span>  <span class="comment"># The name you specified the data to output to in the DUMPGRID command </span></div><div class="line"></div><div class="line"><span class="comment"># Function to read in histogram data and normalization</span></div><div class="line"><span class="keyword">def </span>readhistogram( fname ) :</div><div class="line">        <span class="comment"># Read in the histogram data</span></div><div class="line">        data = np.loadtxt( fname )</div><div class="line">        with open( filename, <span class="stringliteral">&quot;r&quot; ) as myfile :</span></div><div class="line"><span class="stringliteral">                </span><span class="keywordflow">for</span> line <span class="keywordflow">in</span> myfile :</div><div class="line">                        <span class="keywordflow">if</span> line.startswith(<span class="stringliteral">&quot;#! SET normalisation&quot;</span>) : norm = line.split()[3]</div><div class="line">        <span class="keywordflow">return</span> float(norm), data</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># Read in the grid file header to work out what fields we have</span></div><div class="line">with open( filename, <span class="stringliteral">&quot;r&quot; ) as myfile :</span></div><div class="line"><span class="stringliteral">        </span><span class="keywordflow">for</span> line <span class="keywordflow">in</span> myfile :</div><div class="line">                <span class="keywordflow">if</span> line.startswith(<span class="stringliteral">&quot;#! FIELDS&quot;</span>) : fieldnames = line.split()</div><div class="line"></div><div class="line"><span class="comment"># Check if derivatives have been output in the grid by investigating the header</span></div><div class="line">nextg = 1</div><div class="line"><span class="keywordflow">if</span> len(fieldnames)&gt;(2+grid_dimension+nquantities) :</div><div class="line">        nextg = 1 + grid_dimension</div><div class="line">        <span class="keyword">assert</span> len(fieldnames)==(2+grid_dimension + nquantities*nextg)</div><div class="line"></div><div class="line"><span class="comment"># Read in a grid</span></div><div class="line">norm, griddata = readhistogram( filename )</div><div class="line">norm2 = norm*norm</div><div class="line"><span class="comment"># Create two np array that will be used to accumulate the average grid and the average grid squared</span></div><div class="line">average = np.zeros((nquantities, len(griddata[:,0])))</div><div class="line">average_sq = np.zeros((nquantities, len(griddata[:,0])))</div><div class="line"><span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(0,nquantities) :</div><div class="line">        average[i,:] = norm*griddata[:,nquantities+i*nextg]</div><div class="line">        average_sq[i,:] = norm*griddata[:,nquantities+i*nextg]*griddata[:,nquantities+i*nextg]</div><div class="line"></div><div class="line"><span class="comment"># Now sum the grids from all all the analysis files you have</span></div><div class="line"><span class="keywordflow">for</span> filen <span class="keywordflow">in</span> glob.glob( <span class="stringliteral">&quot;analysis.*.&quot;</span> + filename ) :</div><div class="line">        tnorm, newgrid = readhistogram( filen )</div><div class="line">        norm = norm + tnorm</div><div class="line">        norm2 = norm2 + tnorm*tnorm</div><div class="line">        <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(0,nquantities) :</div><div class="line">                average[i,:] = average[i,:] + tnorm*newgrid[:,nquantities+i*nextg]</div><div class="line">                average_sq[i,:] = average_sq[i,:] + tnorm*newgrid[:,nquantities+i*nextg]*newgrid[:,nquantities+i*nextg]</div><div class="line"></div><div class="line"><span class="comment"># Compte the final average grid</span></div><div class="line">average = average / norm</div><div class="line"><span class="comment"># Compute the sample variance for all grid points</span></div><div class="line">variance = (average_sq / norm) - average*average</div><div class="line"><span class="comment"># Now multiply by bessel correction to unbias the sample variance and get the population variance</span></div><div class="line">variance = ( norm /(norm-(norm2/norm)) ) * variance</div><div class="line"><span class="comment"># And lastly divide by number of grids and square root to get an error bar for each grid point</span></div><div class="line">ngrid = 1 + len( glob.glob( <span class="stringliteral">&quot;analysis.*.&quot;</span> + filename ) )</div><div class="line">errors = np.sqrt( variance / ngrid )</div><div class="line"><span class="comment"># Calcualte average error over grid and output in header</span></div><div class="line"><span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(0,nquantities) :</div><div class="line">        mean_error = sum(errors[i,:]) / len(errors[i,:])</div><div class="line">        print(<span class="stringliteral">&quot;# Average error for &quot;</span> + str(i+1) + <span class="stringliteral">&quot;th averaged function on grid equals &quot;</span>, mean_error )</div><div class="line"></div><div class="line"><span class="comment"># Output the final average grid</span></div><div class="line"><span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(0,len(griddata[:,0])) :</div><div class="line">        <span class="keywordflow">for</span> j <span class="keywordflow">in</span> range(0,grid_dimension) : print( griddata[i,j], end=<span class="stringliteral">&quot; &quot;</span> )</div><div class="line">        <span class="keywordflow">for</span> j <span class="keywordflow">in</span> range(0,nquantities) : print( average[j,i], errors[j,i], end=<span class="stringliteral">&quot; &quot;</span> )</div><div class="line">        print()</div></div><!-- fragment --><p>Copy this script to a file called merge-histograms.py and then run its contents by executing the command:</p>
<pre class="fragment">&gt; python merge-histograms.py &gt; final-histogram.dat
</pre><p>This will output the final average histogram together with some error bars. You can plot this function using gnuplot by executing the command:</p>
<pre class="fragment">gnuplot&gt; p 'final-histogram.dat' u 1:2:3 w e, '' u 1:2 w l
</pre><p><b> Where are the error bars in our estimate of the histogram the largest? Why are the errors large in these regions? </b> <br />
 Notice that the file output by the script above also contains information on the average error per grid point in the header. The quantity that is calculated here is:</p>
<p class="formulaDsp">
\[ \textrm{average error} = \frac{1}{N} \sum_{i=1}^N \sigma_i \]
</p>
<p>In this expression \(N\) is the total number of grid points at which the function was evaluated and \(\sigma_i\) is the error bar for the estimate of the function that was calculated for the \(i\)th grid point. The average error is a useful quantity as we can plot it and thus check that our blocks are large enough to correct for the correlation between our data points. In other words, we can use this quantity in the same way that we used the error around the average in <a class="el" href="trieste-2.html#triete-2-correlation">Problem III: Dealing with correlated variables</a>. A plot of the average error versus the size of the blocks that were used in for the block averaging is shown below. This figure demonstrates that a block average length of 500 is certainly long enough to correct for the problems due to the correlations in the values produced at different times: <br />
 <a class="anchor" id="triest-2-ebdata"></a></p><div class="image">
<img src="trieste-2-histogram-errors.png" alt="trieste-2-histogram-errors.png"/>
<div class="caption">
The size of the error bars calculated with different lengths of block average. From this figure it is clear that taking block averages over 500 data points allows us to account for the correlations in the data and extract reasonable error bars.</div></div>
<p> <b> If you have sufficient time try to see if you can reproduce this plot using the data you generated </b></p>
<h1><a class="anchor" id="trieste-2-extensions"></a>
Extensions</h1>
<p>The exercises in the previous sections have shown you how we can calculate ensemble averages from trajectory data. You should have seen that performing block averaging is vital as this technique allows us to deal with the artefacts that we get because of the correlations in the data that we obtain from simulation trajectories. <br />
What we have seen is that when this technique is used correctly we get reasonable error bars. If block averaging is not performed, however, we can underestimate the errors in our data and thus express a false confidence in the reliability of our simulation results. <br />
 The next step for you will be to use this technique when analysing the data from your own simulations. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- HTML footer for doxygen 1.8.3.1-->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
<!--    <li class="navelem"><a class="el" href="tutorials.html">Tutorials</a></li>  -->
    <li class="footer">   <!--- Generated by -->
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.14 </li>
  </ul>
</div>
</body>
</html>
